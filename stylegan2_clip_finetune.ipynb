{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3gPvakXnxW4u7DVdK6vbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdelta/ImageGen/blob/main/stylegan2_clip_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dependencies"
      ],
      "metadata": {
        "id": "pQao9vNHVWOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i18v-FoPKu0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a29717-cf6e-4806-822c-9d0a2615695b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3 open_clip_torch"
      ],
      "metadata": {
        "id": "bbsLKrW85ISu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a03e875-d962-4619-8f0c-f444accc9274"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Collecting pyspng\n",
            "  Downloading pyspng-0.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n",
            "  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 738 kB/s \n",
            "\u001b[?25hCollecting open_clip_torch\n",
            "  Downloading open_clip_torch-2.9.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyspng) (1.21.6)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from open_clip_torch) (0.14.0+cu116)\n",
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from open_clip_torch) (2022.6.2)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from open_clip_torch) (1.13.0+cu116)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 93.4 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9.0->open_clip_torch) (4.4.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->open_clip_torch) (0.2.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->open_clip_torch) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->open_clip_torch) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->open_clip_torch) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.0.9)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->open_clip_torch) (7.1.2)\n",
            "Installing collected packages: sentencepiece, protobuf, huggingface-hub, ftfy, pyspng, open-clip-torch, ninja, imageio-ffmpeg\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\n",
            "Successfully installed ftfy-6.1.1 huggingface-hub-0.11.1 imageio-ffmpeg-0.4.3 ninja-1.11.1 open-clip-torch-2.9.1 protobuf-3.20.3 pyspng-0.1.1 sentencepiece-0.1.97\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl"
      ],
      "metadata": {
        "id": "qYdu-X-YkRIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cb700f-fd98-4fa8-860f-d45fa066dd46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-31 11:06:30--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 18.160.225.32, 18.160.225.53, 18.160.225.119, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|18.160.225.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 295744285 (282M) [binary/octet-stream]\n",
            "Saving to: ‘ffhq-res256-mirror-paper256-noaug.pkl’\n",
            "\n",
            "ffhq-res256-mirror- 100%[===================>] 282.04M  31.1MB/s    in 9.9s    \n",
            "\n",
            "2022-12-31 11:06:41 (28.4 MB/s) - ‘ffhq-res256-mirror-paper256-noaug.pkl’ saved [295744285/295744285]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/datasets/ffhq_256/ffhq.zip ./"
      ],
      "metadata": {
        "id": "_mj9MfXDkUhD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "DFtl-xc6qQ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug code"
      ],
      "metadata": {
        "id": "5H1lulrZVHrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "vfMs5KkzZVc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "suL_JKk1e2KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_images = [image]\n",
        "src_texts = [\"cat\", \"dog\"]\n",
        "images = torch.tensor(np.stack([preprocess(img) for img in src_images])).to(device)\n",
        "texts = tokenizer(src_texts).to(device)"
      ],
      "metadata": {
        "id": "lMBVag3Fe2FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_features = model.encode_text(texts)\n",
        "texts_features /= texts_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "Bm-vCLCjfm5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = model.encode_image(images)\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "iosyOcQXfmwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim = torch.matmul(texts_features, image_features.permute(1, 0))"
      ],
      "metadata": {
        "id": "xNhpKJtRe1_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "id": "pzG2WLOMlccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim"
      ],
      "metadata": {
        "id": "v1R6_cqDfYvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as tfn\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def normalize(x, mean, std):\n",
        "    mean = mean.unsqueeze(1).unsqueeze(2)\n",
        "    std = std.unsqueeze(1).unsqueeze(2)\n",
        "    return (x - mean) / std\n",
        "\n",
        "start = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
        "sized = tfn.interpolate(start, size=224, mode='bicubic')\n",
        "normed = normalize(\n",
        "    sized,\n",
        "    torch.tensor(open_clip.OPENAI_DATASET_MEAN).to(device),\n",
        "    torch.tensor(open_clip.OPENAI_DATASET_STD).to(device)\n",
        ")"
      ],
      "metadata": {
        "id": "h1Ie_4EzkcO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.equal(images, normed)"
      ],
      "metadata": {
        "id": "K5zCyHSAkcM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(images[0].cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "He3tOkPrkcKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(normed[0].cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "rJH6a-tZkcIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_lst = [\n",
        "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(224)\n",
        "]\n",
        "\n",
        "my_preprocess = transforms.Compose(tr_lst)"
      ],
      "metadata": {
        "id": "mtJB4HFtqd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(tr_lst[0](start[0]).cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "GhsI6ko8q0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features.shape"
      ],
      "metadata": {
        "id": "738zIOvZrWZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = start.detach()\n",
        "input.requires_grad_(True)"
      ],
      "metadata": {
        "id": "v0itJEEc450S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSubloss(object):\n",
        "    def __init__(self, device, clip_phrase):\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.model = self.model.to(device)\n",
        "        tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
        "        with torch.no_grad():\n",
        "            self.texts_features = self.model.encode_text(tokenizer([clip_phrase]).to(device))\n",
        "            self.texts_features /= self.texts_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def _preprocess_images(self, images):\n",
        "        resized = torch.nn.functional.interpolate(images, size=224, mode='bicubic')\n",
        "        mean = torch.tensor(open_clip.OPENAI_DATASET_MEAN).to(self.device).unsqueeze(1).unsqueeze(2)\n",
        "        std = torch.tensor(open_clip.OPENAI_DATASET_STD).to(self.device).unsqueeze(1).unsqueeze(2)\n",
        "        return (resized - mean) / std\n",
        "        \n",
        "    def get_similarities(self, images):\n",
        "        images_features = self.model.encode_image(self._preprocess_images(images))\n",
        "        \n",
        "        images_norm = images_features.norm(dim=-1, keepdim=True) + 1e-5\n",
        "        print(images_norm.cpu())\n",
        "        #return (images_features / images_norm).permute(1, 0)\n",
        "        return torch.matmul(self.texts_features, (images_features / images_norm).permute(1, 0))\n",
        "\n",
        "clip_subloss = CLIPSubloss(device, \"glasses\")\n",
        "\n",
        "with torch.autograd.set_detect_anomaly(True):\n",
        "    gen_clip = clip_subloss.get_similarities(input)\n",
        "    gen_clip.mean().mul(4).backward()\n"
      ],
      "metadata": {
        "id": "mTjacEoF3foj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7_xRn6E3fk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYMktYKo3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNzskHab3fb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlfHd2IV3fXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-_m7QL23fSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TIkNmqsrWTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "f_mJY1eykcFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDnxIWbckcCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8elj0ADkb-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNyY0KAZfYnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load repo"
      ],
      "metadata": {
        "id": "w7R_gN5NVLWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -fR stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "t7YNNLG_Y4sy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sdelta/stylegan2-ada-pytorch.git"
      ],
      "metadata": {
        "id": "8RyD4_beY4Hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e975416-1b8e-47ec-b80e-cead883b9026"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 229, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 229 (delta 44), reused 1 (delta 0), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (229/229), 1.15 MiB | 9.09 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune"
      ],
      "metadata": {
        "id": "JNXc-ztnVONd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python stylegan2-ada-pytorch/train.py --outdir=drive/MyDrive/stylegan_finetuning --data=ffhq.zip \\\n",
        "    --mirror=1 --gpus=1 --resume=ffhq-res256-mirror-paper256-noaug.pkl --kimg=1500 --cfg=paper256 \\\n",
        "    --snap=10 --metrics=\"none\" \\\n",
        "    --freezed=10 --freezed_mapping=True \\\n",
        "    --clip_phrase='glasses' --clip_reg_interval=4"
      ],
      "metadata": {
        "id": "fqSSP0b06pqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30663674-86c1-4a6e-d11b-7fe8c0353714"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"ffhq.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 70000,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8,\n",
            "      \"trainable\": false\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 10\n",
            "    },\n",
            "    \"mapping_kwargs\": {\n",
            "      \"trainable\": false\n",
            "    },\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 8\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 1,\n",
            "    \"clip_phrase\": \"glasses\"\n",
            "  },\n",
            "  \"clip_reg_interval\": 4,\n",
            "  \"total_kimg\": 1500,\n",
            "  \"batch_size\": 64,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 20,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"ffhq-res256-mirror-paper256-noaug.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"drive/MyDrive/stylegan_finetuning/00038-ffhq-mirror-paper256-kimg1500-resumecustom-freezed10-freezed_mapping\"\n",
            "}\n",
            "\n",
            "Output directory:   drive/MyDrive/stylegan_finetuning/00038-ffhq-mirror-paper256-kimg1500-resumecustom-freezed10-freezed_mapping\n",
            "Training data:      ffhq.zip\n",
            "Training duration:  1500 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   70000\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  140000\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"ffhq-res256-mirror-paper256-noaug.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape        Datatype\n",
            "---                   ---         ---      ---                 ---     \n",
            "mapping.fc0           -           262656   [8, 512]            float32 \n",
            "mapping.fc1           -           262656   [8, 512]            float32 \n",
            "mapping.fc2           -           262656   [8, 512]            float32 \n",
            "mapping.fc3           -           262656   [8, 512]            float32 \n",
            "mapping.fc4           -           262656   [8, 512]            float32 \n",
            "mapping.fc5           -           262656   [8, 512]            float32 \n",
            "mapping.fc6           -           262656   [8, 512]            float32 \n",
            "mapping.fc7           -           262656   [8, 512]            float32 \n",
            "mapping               -           512      [8, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [8, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [8, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [8, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [8, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [8, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [8, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [8, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [8, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [8, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [8, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [8, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [8, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [8, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [8, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                 ---     \n",
            "Total                 22666210    2276816  -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   -           272      [8, 64, 256, 256]   float16 \n",
            "b256.skip      -           8208     [8, 128, 128, 128]  float16 \n",
            "b256.conv0     -           36944    [8, 64, 256, 256]   float16 \n",
            "b256.conv1     -           73872    [8, 128, 128, 128]  float16 \n",
            "b256           -           16       [8, 128, 128, 128]  float16 \n",
            "b128.skip      -           32784    [8, 256, 64, 64]    float16 \n",
            "b128.conv0     -           147600   [8, 128, 128, 128]  float16 \n",
            "b128.conv1     -           295184   [8, 256, 64, 64]    float16 \n",
            "b128           -           16       [8, 256, 64, 64]    float16 \n",
            "b64.skip       -           131088   [8, 512, 32, 32]    float16 \n",
            "b64.conv0      -           590096   [8, 256, 64, 64]    float16 \n",
            "b64.conv1      -           1180176  [8, 512, 32, 32]    float16 \n",
            "b64            -           16       [8, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]    float16 \n",
            "b32            -           16       [8, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n",
            "b16            -           16       [8, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n",
            "b8             -           16       [8, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [8, 512]            float32 \n",
            "b4.out         513         -        [8, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          21505025    2496480  -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2022-12-31 11:49:34.871812: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Training for 1500 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      time 1m 36s       sec/tick 10.1    sec/kimg 157.18  maintenance 86.2   cpumem 6.83   gpumem 18.03  augment 0.000\n",
            "tick 1     kimg 4.1      time 3m 26s       sec/tick 92.8    sec/kimg 23.02   maintenance 17.5   cpumem 7.11   gpumem 3.99   augment 0.041\n",
            "tick 2     kimg 8.1      time 5m 01s       sec/tick 94.2    sec/kimg 23.37   maintenance 0.0    cpumem 7.11   gpumem 3.99   augment 0.074\n",
            "tick 3     kimg 12.2     time 6m 35s       sec/tick 94.4    sec/kimg 23.40   maintenance 0.0    cpumem 7.11   gpumem 3.99   augment 0.110\n",
            "tick 4     kimg 16.2     time 8m 10s       sec/tick 94.6    sec/kimg 23.46   maintenance 0.0    cpumem 7.11   gpumem 3.99   augment 0.125\n",
            "tick 5     kimg 20.2     time 9m 43s       sec/tick 92.9    sec/kimg 23.03   maintenance 0.0    cpumem 7.12   gpumem 4.00   augment 0.156\n",
            "tick 6     kimg 24.3     time 11m 16s      sec/tick 93.4    sec/kimg 23.16   maintenance 0.0    cpumem 7.12   gpumem 4.00   augment 0.184\n",
            "tick 7     kimg 28.3     time 12m 49s      sec/tick 93.5    sec/kimg 23.19   maintenance 0.0    cpumem 7.12   gpumem 4.01   augment 0.205\n",
            "tick 8     kimg 32.3     time 14m 23s      sec/tick 93.1    sec/kimg 23.09   maintenance 0.0    cpumem 7.12   gpumem 4.02   augment 0.220\n",
            "tick 9     kimg 36.4     time 15m 55s      sec/tick 92.6    sec/kimg 22.97   maintenance 0.0    cpumem 7.12   gpumem 4.00   augment 0.251\n",
            "tick 10    kimg 40.4     time 17m 29s      sec/tick 93.5    sec/kimg 23.19   maintenance 0.0    cpumem 7.12   gpumem 4.03   augment 0.284\n",
            "tick 11    kimg 44.4     time 19m 20s      sec/tick 95.3    sec/kimg 23.65   maintenance 16.3   cpumem 7.82   gpumem 4.02   augment 0.300\n",
            "tick 12    kimg 48.4     time 20m 55s      sec/tick 95.1    sec/kimg 23.59   maintenance 0.0    cpumem 7.82   gpumem 4.01   augment 0.320\n",
            "tick 13    kimg 52.5     time 22m 30s      sec/tick 94.3    sec/kimg 23.39   maintenance 0.0    cpumem 7.82   gpumem 4.02   augment 0.346\n",
            "tick 14    kimg 56.5     time 24m 05s      sec/tick 95.0    sec/kimg 23.55   maintenance 0.0    cpumem 7.82   gpumem 4.05   augment 0.364\n",
            "tick 15    kimg 60.5     time 25m 40s      sec/tick 94.9    sec/kimg 23.53   maintenance 0.0    cpumem 7.82   gpumem 4.04   augment 0.394\n",
            "tick 16    kimg 64.6     time 27m 15s      sec/tick 95.2    sec/kimg 23.61   maintenance 0.0    cpumem 7.82   gpumem 4.04   augment 0.417\n",
            "tick 17    kimg 68.6     time 28m 47s      sec/tick 92.6    sec/kimg 22.98   maintenance 0.0    cpumem 7.82   gpumem 4.04   augment 0.438\n",
            "tick 18    kimg 72.6     time 30m 22s      sec/tick 94.4    sec/kimg 23.41   maintenance 0.0    cpumem 7.82   gpumem 4.05   augment 0.445\n",
            "tick 19    kimg 76.7     time 31m 56s      sec/tick 94.0    sec/kimg 23.31   maintenance 0.0    cpumem 7.82   gpumem 4.08   augment 0.466\n",
            "tick 20    kimg 80.7     time 33m 30s      sec/tick 94.2    sec/kimg 23.36   maintenance 0.0    cpumem 7.82   gpumem 4.03   augment 0.492\n",
            "tick 21    kimg 84.7     time 35m 19s      sec/tick 92.9    sec/kimg 23.05   maintenance 15.9   cpumem 7.82   gpumem 4.04   augment 0.517\n",
            "tick 22    kimg 88.8     time 36m 53s      sec/tick 94.3    sec/kimg 23.39   maintenance 0.0    cpumem 7.83   gpumem 4.03   augment 0.535\n",
            "tick 23    kimg 92.8     time 38m 27s      sec/tick 94.1    sec/kimg 23.34   maintenance 0.0    cpumem 7.83   gpumem 4.04   augment 0.561\n",
            "tick 24    kimg 96.8     time 40m 01s      sec/tick 93.9    sec/kimg 23.29   maintenance 0.0    cpumem 7.83   gpumem 4.04   augment 0.581\n",
            "tick 25    kimg 100.9    time 41m 33s      sec/tick 92.2    sec/kimg 22.87   maintenance 0.0    cpumem 7.83   gpumem 4.04   augment 0.602\n",
            "tick 26    kimg 104.9    time 43m 08s      sec/tick 94.5    sec/kimg 23.45   maintenance 0.0    cpumem 7.83   gpumem 4.05   augment 0.614\n",
            "tick 27    kimg 108.9    time 44m 42s      sec/tick 94.2    sec/kimg 23.36   maintenance 0.0    cpumem 7.83   gpumem 4.05   augment 0.635\n",
            "tick 28    kimg 113.0    time 46m 16s      sec/tick 93.9    sec/kimg 23.30   maintenance 0.0    cpumem 7.83   gpumem 4.05   augment 0.655\n",
            "tick 29    kimg 117.0    time 47m 49s      sec/tick 92.6    sec/kimg 22.97   maintenance 0.0    cpumem 7.83   gpumem 4.05   augment 0.686\n",
            "tick 30    kimg 121.0    time 49m 23s      sec/tick 94.7    sec/kimg 23.49   maintenance 0.0    cpumem 7.83   gpumem 4.05   augment 0.694\n",
            "tick 31    kimg 125.1    time 51m 15s      sec/tick 96.2    sec/kimg 23.87   maintenance 15.5   cpumem 7.83   gpumem 4.05   augment 0.719\n",
            "tick 32    kimg 129.1    time 52m 51s      sec/tick 95.5    sec/kimg 23.70   maintenance 0.0    cpumem 7.83   gpumem 4.07   augment 0.750\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "Failure: not much more people with glasses from generator. Furthermore starting from 30 tick imagegrid show signs of mode collapse - all faces have similar structure"
      ],
      "metadata": {
        "id": "RbzaptMJstgn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMhvOglOqN0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}