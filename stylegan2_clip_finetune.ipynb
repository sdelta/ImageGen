{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPea0mLgv1ck2KtIYERZ0bd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdelta/ImageGen/blob/main/stylegan2_clip_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i18v-FoPKu0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3 open_clip_torch"
      ],
      "metadata": {
        "id": "bbsLKrW85ISu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl"
      ],
      "metadata": {
        "id": "qYdu-X-YkRIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/datasets/ffhq_256/ffhq.zip ./"
      ],
      "metadata": {
        "id": "_mj9MfXDkUhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "DFtl-xc6qQ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "vfMs5KkzZVc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "suL_JKk1e2KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_images = [image]\n",
        "src_texts = [\"cat\", \"dog\"]\n",
        "images = torch.tensor(np.stack([preprocess(img) for img in src_images])).to(device)\n",
        "texts = tokenizer(src_texts).to(device)"
      ],
      "metadata": {
        "id": "lMBVag3Fe2FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_features = model.encode_text(texts)\n",
        "texts_features /= texts_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "Bm-vCLCjfm5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = model.encode_image(images)\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "iosyOcQXfmwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim = torch.matmul(texts_features, image_features.permute(1, 0))"
      ],
      "metadata": {
        "id": "xNhpKJtRe1_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "id": "pzG2WLOMlccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim"
      ],
      "metadata": {
        "id": "v1R6_cqDfYvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as tfn\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def normalize(x, mean, std):\n",
        "    mean = mean.unsqueeze(1).unsqueeze(2)\n",
        "    std = std.unsqueeze(1).unsqueeze(2)\n",
        "    return (x - mean) / std\n",
        "\n",
        "start = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
        "sized = tfn.interpolate(start, size=224, mode='bicubic')\n",
        "normed = normalize(\n",
        "    sized,\n",
        "    torch.tensor(open_clip.OPENAI_DATASET_MEAN).to(device),\n",
        "    torch.tensor(open_clip.OPENAI_DATASET_STD).to(device)\n",
        ")"
      ],
      "metadata": {
        "id": "h1Ie_4EzkcO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.equal(images, normed)"
      ],
      "metadata": {
        "id": "K5zCyHSAkcM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(images[0].cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "He3tOkPrkcKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(normed[0].cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "rJH6a-tZkcIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_lst = [\n",
        "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(224)\n",
        "]\n",
        "\n",
        "my_preprocess = transforms.Compose(tr_lst)"
      ],
      "metadata": {
        "id": "mtJB4HFtqd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(tr_lst[0](start[0]).cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "GhsI6ko8q0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features.shape"
      ],
      "metadata": {
        "id": "738zIOvZrWZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = start.detach()\n",
        "input.requires_grad_(True)"
      ],
      "metadata": {
        "id": "v0itJEEc450S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSubloss(object):\n",
        "    def __init__(self, device, clip_phrase):\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.model = self.model.to(device)\n",
        "        tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
        "        with torch.no_grad():\n",
        "            self.texts_features = self.model.encode_text(tokenizer([clip_phrase]).to(device))\n",
        "            self.texts_features /= self.texts_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def _preprocess_images(self, images):\n",
        "        resized = torch.nn.functional.interpolate(images, size=224, mode='bicubic')\n",
        "        mean = torch.tensor(open_clip.OPENAI_DATASET_MEAN).to(self.device).unsqueeze(1).unsqueeze(2)\n",
        "        std = torch.tensor(open_clip.OPENAI_DATASET_STD).to(self.device).unsqueeze(1).unsqueeze(2)\n",
        "        return (resized - mean) / std\n",
        "        \n",
        "    def get_similarities(self, images):\n",
        "        images_features = self.model.encode_image(self._preprocess_images(images))\n",
        "        \n",
        "        images_norm = images_features.norm(dim=-1, keepdim=True) + 1e-5\n",
        "        print(images_norm.cpu())\n",
        "        #return (images_features / images_norm).permute(1, 0)\n",
        "        return torch.matmul(self.texts_features, (images_features / images_norm).permute(1, 0))\n",
        "\n",
        "clip_subloss = CLIPSubloss(device, \"glasses\")\n",
        "\n",
        "with torch.autograd.set_detect_anomaly(True):\n",
        "    gen_clip = clip_subloss.get_similarities(input)\n",
        "    gen_clip.mean().mul(4).backward()\n"
      ],
      "metadata": {
        "id": "mTjacEoF3foj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7_xRn6E3fk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYMktYKo3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNzskHab3fb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlfHd2IV3fXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-_m7QL23fSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TIkNmqsrWTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "f_mJY1eykcFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDnxIWbckcCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8elj0ADkb-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNyY0KAZfYnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -fR stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "t7YNNLG_Y4sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sdelta/stylegan2-ada-pytorch.git"
      ],
      "metadata": {
        "id": "8RyD4_beY4Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python stylegan2-ada-pytorch/train.py --outdir=drive/MyDrive/stylegan_finetuning --data=ffhq.zip \\\n",
        "    --mirror=1 --gpus=1 --resume=ffhq-res256-mirror-paper256-noaug.pkl --kimg=1500 --cfg=paper256 \\\n",
        "    --freezed=10 --freezed_mapping=True \\\n",
        "    --clip_phrase='glasses' --clip_reg_interval=4"
      ],
      "metadata": {
        "id": "fqSSP0b06pqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMhvOglOqN0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}